{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RJOPOJg3Owv0"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fTgU9fbQYZK"
   },
   "source": [
    "# Step 1: Generating the Synthetic Dataset\n",
    "\n",
    "We'll generate synthetic data in three parts:\n",
    "\n",
    "- User Data: Simulate user demographic details.\n",
    "- Product Data: Simulate product details.\n",
    "- Interaction Data: Simulate how users interact with products (e.g., purchase or rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6OUrb2gQnX4",
    "outputId": "0acf3b48-34c9-44d5-ebf5-d570fb3d6a4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   user_id  age gender location\n",
       " 0        1   35      M    Rural\n",
       " 1        2   37      F    Urban\n",
       " 2        3   21      M    Urban\n",
       " 3        4   50      F    Rural\n",
       " 4        5   28      M    Urban,\n",
       "    product_id     category   price  rating\n",
       " 0           1        Books  187.70     3.9\n",
       " 1           2  Electronics  275.56     4.4\n",
       " 2           3     Clothing  409.03     2.8\n",
       " 3           4        Books  327.38     2.1\n",
       " 4           5  Electronics  184.25     4.8,\n",
       "    user_id  product_id  rating           timestamp\n",
       " 0      179         278       5 2023-01-01 00:00:00\n",
       " 1      817          72       2 2023-01-01 00:01:00\n",
       " 2      533         366       4 2023-01-01 00:02:00\n",
       " 3      716         306       3 2023-01-01 00:03:00\n",
       " 4      806         202       3 2023-01-01 00:04:00)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # To handle data in tabular form\n",
    "import numpy as np   # To generate random data\n",
    "\n",
    "# Step 1: Define the number of users and products\n",
    "# Let's assume we have 1000 users and 500 products in our ecommerce platform.\n",
    "num_users = 1000\n",
    "num_products = 500\n",
    "\n",
    "# Step 2: Generating the Users Data\n",
    "# Each user has an ID, age, gender, and location.\n",
    "user_data = {\n",
    "    'user_id': np.arange(1, num_users + 1),  # Generate user IDs from 1 to 1000\n",
    "    'age': np.random.randint(18, 70, size=num_users),  # Random ages between 18 and 70\n",
    "    'gender': np.random.choice(['M', 'F'], size=num_users),  # Randomly assign gender as Male (M) or Female (F)\n",
    "    'location': np.random.choice(['Urban', 'Suburban', 'Rural'], size=num_users)  # Randomly assign location type\n",
    "}\n",
    "\n",
    "# Convert the user data dictionary into a pandas DataFrame\n",
    "users_df = pd.DataFrame(user_data)\n",
    "\n",
    "# Step 3: Generating the Products Data\n",
    "# Each product has an ID, category, price, and rating.\n",
    "product_data = {\n",
    "    'product_id': np.arange(1, num_products + 1),  # Generate product IDs from 1 to 500\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], size=num_products),  # Randomly assign product category\n",
    "    'price': np.round(np.random.uniform(5, 500, size=num_products), 2),  # Random prices between $5 and $500, rounded to 2 decimal places\n",
    "    'rating': np.round(np.random.uniform(1, 5, size=num_products), 1)  # Random ratings between 1 and 5, rounded to 1 decimal place\n",
    "}\n",
    "\n",
    "# Convert the product data dictionary into a pandas DataFrame\n",
    "products_df = pd.DataFrame(product_data)\n",
    "\n",
    "# Step 4: Generating the User-Product Interaction Data (Purchase History or Ratings)\n",
    "# We simulate how users interact with products. For example, users can rate or buy products.\n",
    "\n",
    "interaction_data = {\n",
    "    'user_id': np.random.choice(users_df['user_id'], size=5000),  # Randomly select users who interacted with products\n",
    "    'product_id': np.random.choice(products_df['product_id'], size=5000),  # Randomly select products that were interacted with\n",
    "    'rating': np.random.randint(1, 6, size=5000),  # Assign random ratings (1 to 5 stars) for these interactions\n",
    "    'timestamp': pd.date_range(start='2023-01-01', periods=5000, freq='T')  # Generate random timestamps for interactions, 1 minute apart\n",
    "}\n",
    "\n",
    "# Convert the interaction data dictionary into a pandas DataFrame\n",
    "interactions_df = pd.DataFrame(interaction_data)\n",
    "\n",
    "# Let's check the first few rows of each dataset\n",
    "users_df.head(), products_df.head(), interactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfZB0nkpQtGn"
   },
   "source": [
    "# Explanation:\n",
    "1. Libraries:\n",
    "\n",
    "- pandas: Used for handling tabular data (like spreadsheets).\n",
    "numpy: Helps generate random values for simulating user, product, and interaction data.\n",
    "\n",
    "2. User Data:\n",
    "\n",
    "- We create 1000 users with random ages, genders (male/female), and locations (urban/suburban/rural).\n",
    "3. Product Data:\n",
    "\n",
    "- We create 500 products, each belonging to a random category (Electronics, Clothing, Home, Books), with random prices and ratings.\n",
    "\n",
    "4. Interaction Data:\n",
    "\n",
    "- We simulate 5000 interactions between users and products, each with a rating (1 to 5 stars) and a timestamp to capture when the interaction happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5B2GTBRRG0R"
   },
   "source": [
    "# Step 2: Data Pre-processing.\n",
    "\n",
    "Data pre-processing involves preparing the data for training by handling missing values, encoding categorical data, and normalizing or scaling numerical data, if needed. In the context of a recommendation system, we mainly focus on:\n",
    "\n",
    "1. Handling missing values: Ensure no missing data in users, products, or interactions.\n",
    "2. Encoding categorical variables: Convert categories like gender or category into numerical format.\n",
    "3. Creating a user-product matrix: This will be the input for our recommendation model, where each row represents a user, each column represents a product, and the cells represent ratings or interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8m1BQqAQosb",
    "outputId": "b148560c-3483-453d-9daa-9f13991daa76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\InsightCart\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.21.6)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in users data:\n",
      " user_id     0\n",
      "age         0\n",
      "gender      0\n",
      "location    0\n",
      "dtype: int64\n",
      "Missing values in products data:\n",
      " product_id    0\n",
      "category      0\n",
      "price         0\n",
      "rating        0\n",
      "dtype: int64\n",
      "Missing values in interactions data:\n",
      " user_id       0\n",
      "product_id    0\n",
      "rating        0\n",
      "timestamp     0\n",
      "dtype: int64\n",
      "User-Product Matrix:\n",
      " product_id  1    2    3    4    5    6    7    8    9    10   ...  491  492  \\\n",
      "user_id                                                       ...             \n",
      "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "5           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "product_id  493  494  495  496  497  498  499  500  \n",
      "user_id                                             \n",
      "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4           0.0  0.0  0.0  0.0  0.0  4.0  0.0  0.0  \n",
      "5           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 500 columns]\n",
      "Train Data Sample:\n",
      "       user_id  product_id  rating           timestamp\n",
      "4227      580          16       1 2023-01-03 22:27:00\n",
      "4676      531         264       4 2023-01-04 05:56:00\n",
      "800       412         259       4 2023-01-01 13:20:00\n",
      "3671      216         418       2 2023-01-03 13:11:00\n",
      "4193      651         322       5 2023-01-03 21:53:00\n",
      "Test Data Sample:\n",
      "       user_id  product_id  rating           timestamp\n",
      "1501      303         106       2 2023-01-02 01:01:00\n",
      "2586      830         489       4 2023-01-02 19:06:00\n",
      "2653      636         155       3 2023-01-02 20:13:00\n",
      "1055        6         406       4 2023-01-01 17:35:00\n",
      "705       842         144       1 2023-01-01 11:45:00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "# Checking for missing values in all datasets\n",
    "print(\"Missing values in users data:\\n\", users_df.isnull().sum())\n",
    "print(\"Missing values in products data:\\n\", products_df.isnull().sum())\n",
    "print(\"Missing values in interactions data:\\n\", interactions_df.isnull().sum())\n",
    "\n",
    "# If there were any missing values, we would handle them here. Since this is synthetic data, it’s unlikely.\n",
    "\n",
    "# Step 2: Encoding categorical variables\n",
    "# We need to convert categorical variables like 'gender' and 'category' into numerical format.\n",
    "# Using LabelEncoder to encode these categorical features\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the gender column in users data (M -> 0, F -> 1)\n",
    "users_df['gender_encoded'] = label_encoder.fit_transform(users_df['gender'])\n",
    "\n",
    "# Encode the location column in users data\n",
    "users_df['location_encoded'] = label_encoder.fit_transform(users_df['location'])\n",
    "\n",
    "# Encode the category column in products data\n",
    "products_df['category_encoded'] = label_encoder.fit_transform(products_df['category'])\n",
    "\n",
    "# Step 3: Create a User-Product Rating Matrix\n",
    "# We pivot the interactions data to create a matrix with users as rows, products as columns, and ratings as values.\n",
    "user_product_matrix = interactions_df.pivot_table(index='user_id', columns='product_id', values='rating').fillna(0)\n",
    "\n",
    "# Step 4: Train-test split\n",
    "# We will split the user-product interaction data into training and test sets.\n",
    "train_data, test_data = train_test_split(interactions_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Let's display the first few rows of the pre-processed data to verify\n",
    "print(\"User-Product Matrix:\\n\", user_product_matrix.head())\n",
    "print(\"Train Data Sample:\\n\", train_data.head())\n",
    "print(\"Test Data Sample:\\n\", test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDnblceNRVb4"
   },
   "source": [
    "# Explanation:\n",
    "\n",
    "1. Handling Missing Values:\n",
    "\n",
    "- We check for missing values using isnull().sum(). Since our data is synthetic, there should be no missing values, but in real-world data, you'd handle this by filling missing values or dropping them.\n",
    "\n",
    "2. Encoding Categorical Variables:\n",
    "\n",
    "- We convert categorical variables such as gender, location, and category into numerical format using LabelEncoder, which is important for machine learning models to understand non-numerical data.\n",
    "\n",
    "3. Creating User-Product Matrix:\n",
    "\n",
    "- We use a pivot table to transform the interactions_df into a matrix where rows represent users, columns represent products, and values represent the rating. This matrix is what we’ll use to train our recommendation model.\n",
    "\n",
    "4. Train-test Split:\n",
    "\n",
    "- We split the interaction data into training (80%) and testing (20%) sets to evaluate our model later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCGS1W5JRo-m"
   },
   "source": [
    "# Model Training and Testing.\n",
    "\n",
    "We’ll implement a Collaborative Filtering recommendation system using the Surprise library, which is popular for building recommendation systems. The algorithm we’ll use is Singular Value Decomposition (SVD), a matrix factorization technique commonly used for collaborative filtering.\n",
    "\n",
    "Here’s what we’ll do in this step:\n",
    "\n",
    "- Install and Import the Surprise library.\n",
    "- Prepare the data for the Surprise library.\n",
    "- Train the SVD model using the training dataset.\n",
    "- Evaluate the model on the test dataset using metrics such as RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqD8Su8aRQy5",
    "outputId": "c746ca9d-cc7a-4f08-b390-83a7d2b79731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4711\n"
     ]
    }
   ],
   "source": [
    "# Install the Surprise library\n",
    "# !pip install scikit-surprise\n",
    "\n",
    "# Import necessary libraries\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "\n",
    "# Step 1: Prepare the data for Surprise\n",
    "# Surprise expects data to have 3 columns: user_id, product_id, and rating. We'll use the interaction data for this.\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))  # The rating scale in our dataset is from 1 to 5\n",
    "data = Dataset.load_from_df(interactions_df[['user_id', 'product_id', 'rating']], reader)\n",
    "\n",
    "# Step 2: Train-test split\n",
    "# We perform a 80/20 train-test split on the Surprise dataset\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Step 3: Train the SVD model\n",
    "# SVD is a matrix factorization technique used for collaborative filtering\n",
    "model = SVD()  # Initialize the SVD model\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(trainset)\n",
    "\n",
    "# Step 4: Test the model on the test set\n",
    "# We predict the ratings for the test set and evaluate performance\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Step 5: Evaluate the performance using RMSE\n",
    "rmse = accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWs8qZDMR0GF"
   },
   "source": [
    "# Explanation:\n",
    "\n",
    "1. Installing the Surprise Library:\n",
    "\n",
    "- We use the Surprise library, which is specifically designed for building recommendation systems. It supports various algorithms, including SVD, KNN, and more.\n",
    "\n",
    "2. Preparing Data:\n",
    "\n",
    "- We format the interaction data (user_id, product_id, rating) in a way that the Surprise library understands using the Reader class.\n",
    "\n",
    "3. Train-test Split:\n",
    "\n",
    "- We split the data into training and test sets (80/20 split) using surprise_train_test_split().\n",
    "\n",
    "4. Training the SVD Model:\n",
    "\n",
    "- We initialize the SVD model and train it on the training set. The SVD algorithm performs matrix factorization, which is suitable for collaborative filtering tasks.\n",
    "\n",
    "5. Evaluating the Model:\n",
    "\n",
    "- After training the model, we test it on the test set. We calculate the RMSE (Root Mean Squared Error), which tells us how well the model is predicting user ratings.\n",
    "\n",
    "**The RMSE (Root Mean Squared Error) of your model is 1.4730, which indicates the average error between the predicted and actual ratings. This score is a good starting point, but there's room for improvement by tuning the model or using other techniques like incorporating more data or trying different algorithms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO49sCGVSV2R"
   },
   "source": [
    "# Step 4: Saving the Model\n",
    "- We will use Python's pickle library to serialize (save) the model to a file. This way, you can load the model in your Flask app and use it to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSuC5wK8SVWM",
    "outputId": "264ad0c1-6be6-49a1-848a-88f25bae05d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model\\svd_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Step 1: Save the trained SVD model to a file\n",
    "model_filename = 'svd_model.pkl'\n",
    "\n",
    "# Create the \"model\" directory if it doesn't exist\n",
    "model_dir = \"model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Join the components into a full path\n",
    "model_path = os.path.join(model_dir, model_filename)\n",
    "\n",
    "with open(model_path, 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj2f02HOSeeW"
   },
   "source": [
    "# Explanation:\n",
    "1. Pickle Library: This is used to serialize the Python objects (in this case, the trained model) into a file.\n",
    "2. Saving the Model: We open a file in write-binary mode (wb) and save the model to svd_model.pkl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD1IihAMS_mt"
   },
   "source": [
    "# Step 5: Monitoring the Model Performance\n",
    "- Let’s extend the RMSE evaluation by calculating MAE (Mean Absolute Error) and generating a performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lkf3goSPSuph",
    "outputId": "48b31a6a-01ae-4041-b9bb-76ef5b6f0b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.2682\n",
      "Model Performance Report:\n",
      "RMSE: 1.4711\n",
      "MAE: 1.2682\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate MAE (Mean Absolute Error)\n",
    "mae = accuracy.mae(predictions)\n",
    "\n",
    "# Step 2: Generate a basic performance report\n",
    "performance_report = {\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae\n",
    "}\n",
    "\n",
    "# Display the performance report\n",
    "print(\"Model Performance Report:\")\n",
    "for metric, score in performance_report.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCxhv35ATF7h"
   },
   "source": [
    "# Explanation:\n",
    "\n",
    "1. MAE (Mean Absolute Error):\n",
    "\n",
    "- While RMSE penalizes larger errors more, MAE gives an average of absolute errors. Both metrics are useful for understanding the model's performance.\n",
    "\n",
    "2. Performance Report:\n",
    "\n",
    "- We store and print both RMSE and MAE, which gives you an overview of how the model is performing.\n",
    "\n",
    "# Additional Step: Advanced Monitoring (Optional)\n",
    "If you want to go beyond basic monitoring, you can:\n",
    "\n",
    "1. Track performance over time: You can log the performance after every training cycle to see if the model improves or worsens.\n",
    "2. Visualization: You can create plots showing error distribution, or monitor the recommendation accuracy using recall/precision if you have ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkOiaKBZTDsp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
